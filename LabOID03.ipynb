{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4R2YG3FsVCS"
      },
      "source": [
        "### <span style=\"color:rgb(92,29,79)\">Optimization for Machine Learning</span>\n",
        "\n",
        "### <span style=\"color:rgb(92,29,79)\">Master 2 ID Apprentissage 2024-2025</span>\n",
        "\n",
        "\n",
        "# <span style=\"color:rgb(92,29,79)\">Lab 03 - Stochastic gradient</span>\n",
        "\n",
        "Lecture notes for this course are available [here](https://www.lamsade.dauphine.fr/~croyer/ensdocs/OID/PolyOID.pdf).\n",
        "\n",
        "This Jupyter notebook can be obtained [here](https://www.lamsade.dauphine.fr/~croyer/ensdocs/OID/LabOID03.zip). The solution notebook will be available after the lab session [here](https://www.lamsade.dauphine.fr/~croyer/ensdocs/OID/LabOID03-solutions.zip).\n",
        "\n",
        "\n",
        "For any comment regarding this notebook (including typos), please send an email to:  **clement.royer@lamsade.dauphine.fr**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV7ze-PasVCW"
      },
      "source": [
        "# <span style=\"color:rgb(92,29,79)\">Introduction</span>\n",
        "\n",
        "The goal of this session is to illustrate and confirm the interest of stochastic gradient techniques on finite-sum problems. To this end, we will rely on the finite-sum optimization problems with synthetic data that we already considered in the previous lab sessions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S5D34KfTsVCW"
      },
      "outputs": [],
      "source": [
        "# Import useful librairies and functions\n",
        "###############################################\n",
        "\n",
        "# Plots\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from math import sqrt # Square root\n",
        "\n",
        "# NumPy - Vector and matrix structures\n",
        "import numpy as np # NumPy library\n",
        "from numpy.random import multivariate_normal, randn, uniform, choice # Probability distributions\n",
        "\n",
        "\n",
        "# SciPy - Efficient numerical calculations\n",
        "from scipy.linalg import norm # Standard norms\n",
        "from scipy.linalg import toeplitz # Toeplitz matrices\n",
        "from scipy.linalg import svdvals # Singular value decomposition (recall Lecture 2!)\n",
        "from scipy.optimize import check_grad # Numerical check of derivatives\n",
        "from scipy.optimize import fmin_l_bfgs_b # An efficient minimization routine in moderate dimensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuOLPPDpsVCY"
      },
      "source": [
        "# <span style=\"color:rgb(92,29,79)\">Part 1 - Data and finite-sum formulations</span>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcSHEnCNsVCY"
      },
      "source": [
        "## <span style=\"color:rgb(92,29,79)\">1.1 - Data generation</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i02copSJsVCZ"
      },
      "source": [
        "Following the lectures, we consider a dataset $\\{(\\mathbf{x}_i,y_i)\\}_{i=1,\\dots,n}$, where $\\mathbf{x}_i \\in \\mathbb{R}^d$ and $y_i \\in \\mathbb{R}$ under the form of\n",
        "\n",
        "- a feature matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$;\n",
        "- a label vector $\\mathbf{y} \\in \\mathbb{R}^n$.\n",
        "\n",
        "Our goal is to learn a mapping from $\\mathbf{X}$ to $\\mathbf{y}$. We consider that such a mapping or model, denoted by $h$, is parameterized by a vector $\\mathbf{w}$, and the accuracy of the model is measured according to a loss function $\\ell$. As a result, we will consider problems of the form\n",
        "$$\n",
        "    \\mathrm{minimize}_{\\mathbf{w} \\in \\mathbb{R}^d} f(\\mathbf{w}) = \\frac{1}{n} \\sum_{i=1}^n f_i(\\mathbf{w}), \\qquad f_i(\\mathbf{w}) = \\ell(h(\\mathbf{x}_i;\\mathbf{w}),y_i) + \\frac{\\lambda}{2}\\|\\mathbf{w}\\|^2.\n",
        "$$\n",
        "where $\\lambda \\ge 0$ is an optional regularization parameter.\n",
        "\n",
        "The dataset will be generated using the following procedure, that promotes correlation between the samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CKSW0iOpsVCZ"
      },
      "outputs": [],
      "source": [
        "# Generate data for linear models. This code is based on a generator proposed by Alexandre Gramfort (INRIA).\n",
        "def simu_lin(wtruth, n, std=1., corr=0.5):\n",
        "    \"\"\"\n",
        "    Generation of data from a linear trend corrupted by Gaussian noise.\n",
        "\n",
        "    Inputs\n",
        "    ----------\n",
        "    wtruth: np.ndarray, shape=(d,)\n",
        "        Ground truth coefficients\n",
        "\n",
        "    n: int\n",
        "        Sample size\n",
        "\n",
        "    std: float, default=1.\n",
        "        Standard deviation for the noise\n",
        "\n",
        "    corr: float, default=0.5\n",
        "        Correlation for the feature matrix\n",
        "\n",
        "    Outputs\n",
        "    ------------\n",
        "    X: Feature matrix\n",
        "    y: Label vector\n",
        "\n",
        "    \"\"\"\n",
        "    d = wtruth.shape[0]\n",
        "    cov = toeplitz(corr ** np.arange(0, d))\n",
        "    X = multivariate_normal(np.zeros(d), cov, size=n)\n",
        "    noise = std * randn(n)\n",
        "    # Main relationship\n",
        "    y = X.dot(wtruth) + noise\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI1rmnAbsVCa"
      },
      "source": [
        "The data is generated by a linear ground truth plus (Gaussian) noise, resulting in data this is quite favorable to stochastic gradient techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NqRbQHlsVCa"
      },
      "source": [
        "## <span style=\"color:rgb(92,29,79)\">1.2 Linear regression</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dolKc-opsVCa"
      },
      "source": [
        "We first investigate the prediction of $y_i$ from $\\mathbf{x}_i$ using a linear model, giving rise to the following linear least-squares optimization problem:\n",
        "$$\n",
        "    \\mathrm{minimize}_{\\mathbf{w} \\in \\mathbb{R}^d} f(\\mathbf{w})\n",
        "    := \\frac{1}{2 n} \\|\\mathbf{X} \\mathbf{w} - \\mathbf{y}\\|^2 + \\frac{\\lambda}{2}\\|\\mathbf{w}\\|^2.\n",
        "$$\n",
        "Note that the objective function of this problem is slightly modified compared to the one we studied in previous lab sessions. Nevertheless, it remains a linear least-squares objective and, as such, it is $\\mathcal{C}^1$.\n",
        "\n",
        "Moreover, this problem has a finite-sum structure $f(\\mathbf{w})=\\tfrac{1}{n}\\sum_{i=1}^n f_i(\\mathbf{w})$, where\n",
        "$$\n",
        "    f_i(\\mathbf{w}) = \\frac{1}{2} (\\mathbf{x}_i^T \\mathbf{w} - y_i)^2 + \\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2.\n",
        "$$\n",
        "One can show that all $f_i$s are $\\mathcal{C}^1$, which implies that $f$ is also $\\mathcal{C}^1$. In addition, one has\n",
        "$$\n",
        "    \\nabla f_i(\\mathbf{w}) = (\\mathbf{x}_i^T \\mathbf{w} - y_i)\\mathbf{x}_i + \\lambda \\mathbf{w}\n",
        "$$\n",
        "for any $i=1,\\dots,n$ and\n",
        "$$\n",
        "    \\nabla f(\\mathbf{w}) =\\frac{1}{n}\\mathbf{X}^T (\\mathbf{X} \\mathbf{w} - \\mathbf{y}) + \\lambda\\mathbf{w}.\n",
        "$$\n",
        "Finally, $\\nabla f$ is $\\left(\\frac{\\|\\mathbf{X}^T \\mathbf{X}\\|}{n}+\\lambda\\right)$-Lipschitz continuous."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUhhwFY-sVCa"
      },
      "source": [
        "## <span style=\"color:rgb(92,29,79)\">1.3 Logistic regression</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3ej6aozsVCb"
      },
      "source": [
        "In logistic regression, we aim at predicting a binary label (in our case $y_i \\in \\{-1,1\\}$ ) from $\\mathbf{x}_i$ using a linear model (or rather the sign of a linear function) of $\\mathbf{x}_i$.\n",
        "\n",
        "These considerations give rise to the following model:\n",
        "$$\n",
        "    \\mathrm{minimize}_{\\mathbf{w} \\in \\mathbb{R}^d} f(\\mathbf{w})\n",
        "    := \\frac{1}{n} \\sum_{i=1}^n f_i(\\mathbf{w}), \\qquad\n",
        "    f_i(\\mathbf{w})=\\log(1+\\exp(-y_i \\mathbf{x}_i^T \\mathbf{w}))+\\frac{\\lambda}{2}\\|\\mathbf{w}\\|^2,\n",
        "$$\n",
        "where every $y_i$ belong to$\\{-1,1\\}$ and the second term in the objective is again a regularization one.\n",
        "\n",
        "This problem has a natural finite-sum structure, and we can show that $f_i \\in \\mathcal{C}^1(\\mathbb{R}^d)$ for all $i$ as well as $f \\in \\mathcal{C}^{1,1}_L(\\mathbb{R}^d)$, where\n",
        "$$\n",
        "   \\forall i=1,\\dots,n, \\quad \\nabla f_i(\\mathbf{w}) = - \\frac{y_i}{1 + \\exp(y_i \\mathbf{x}_i^T \\mathbf{w})} \\mathbf{x}_i + \\lambda \\mathbf{w},\n",
        "$$\n",
        "$$\n",
        "\\nabla f(\\mathbf{w}) = - \\frac{1}{n} \\sum_{i=1}^n\\frac{y_i}{1 + \\exp(y_i \\mathbf{x}_i^T \\mathbf{w})} \\mathbf{x}_i + \\lambda \\mathbf{w}\n",
        "$$\n",
        "and\n",
        "$$\n",
        "    L=\\frac{\\|\\mathbf{X}^T \\mathbf{X}\\|}{4n}+\\lambda.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQkukfXzsVCb"
      },
      "source": [
        "## <span style=\"color:rgb(92,29,79)\">1.4 Python class for regression problems</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ageiq6Q-sVCb"
      },
      "source": [
        "The code below implements regression problems while accounting for their finite-sum structure.\n",
        "\n",
        "*Recall:* For any two-dimensional NumPy array ``X``, ``X[i]`` denotes the $i$th row of ``X``. For any vector ``x``, ``x.dot(v)`` denotes the scalar product between ``x`` and ``v``, while ``x.T`` is the transpose of ``x``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0R7azX7MsVCb"
      },
      "outputs": [],
      "source": [
        "class RegPb(object):\n",
        "    '''\n",
        "        A class for linear and logistic regression based on linear models.\n",
        "\n",
        "        Describes an optimization problem of the form\n",
        "\n",
        "            minimize_w loss(w) + lbda*||w||^2,\n",
        "\n",
        "        where loss is a given function that is defined based on data.\n",
        "\n",
        "        Attributes:\n",
        "            X: Feature matrix\n",
        "            y: Label vector\n",
        "            n,d: Dimensions of X\n",
        "            loss: Loss function\n",
        "                'l2': Least-squares loss\n",
        "                'logit': Logistic loss based on softplus\n",
        "            lbda: \"Strong convexification\"/Regularization parameter\n",
        "\n",
        "        Methods:\n",
        "            fun: Returns the objective value at a given point w\n",
        "            grad: Returns the gradient value at a given point w\n",
        "            lipgrad: Lipschitz constant for the gradient\n",
        "            cvxval: Returns the strong convexity constant if the objective function is strongly convex, 0 otherwise.\n",
        "\n",
        "    '''\n",
        "\n",
        "    # Instantiate the class\n",
        "    def __init__(self, X, y,lbda=0,loss='l2'):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.n, self.d = X.shape\n",
        "        self.loss = loss\n",
        "        self.lbda = lbda\n",
        "\n",
        "\n",
        "    #Objective function\n",
        "    def fun(self, w):\n",
        "        if self.loss=='l2':\n",
        "            return norm(self.X.dot(w) - self.y) ** 2 / (2. * self.n) + self.lbda * norm(w) ** 2 / 2.\n",
        "        elif self.loss=='logit':\n",
        "            yXw = self.y * self.X.dot(w)\n",
        "            return np.mean(np.log(1. + np.exp(-yXw))) + self.lbda * norm(w) ** 2 / 2.\n",
        "\n",
        "    # Value of one term in the sum\n",
        "    def f_i(self, i, w):\n",
        "        if self.loss=='l2':\n",
        "            return norm(self.X[i].dot(w) - self.y[i]) ** 2 / (2.) + self.lbda * norm(w) ** 2 / 2.\n",
        "        elif self.loss=='logit':\n",
        "            yXwi = self.y[i] * np.dot(self.X[i], w)\n",
        "            return np.log(1. + np.exp(- yXwi)) + self.lbda * norm(w) ** 2 / 2.\n",
        "\n",
        "    # Gradient of the objective\n",
        "    def grad(self, w):\n",
        "        if self.loss=='l2':\n",
        "            return self.X.T.dot(self.X.dot(w) - self.y) / self.n + self.lbda * w\n",
        "        elif self.loss=='logit':\n",
        "            yXw = self.y * self.X.dot(w)\n",
        "            aux = 1. / (1. + np.exp(yXw))\n",
        "            return - (self.X.T).dot(self.y * aux) / self.n + self.lbda * w\n",
        "\n",
        "    # Gradient of one term in the finite sum\n",
        "    def grad_i(self,i,w):\n",
        "        x_i = self.X[i]\n",
        "        if self.loss=='l2':\n",
        "            return (x_i.dot(w) - self.y[i]) * x_i + self.lbda*w\n",
        "        elif self.loss=='logit':\n",
        "            grad = - x_i * self.y[i] / (1. + np.exp(self.y[i]* x_i.dot(w)))\n",
        "            grad += self.lbda * w\n",
        "            return grad\n",
        "\n",
        "    # Lipschitz constant for the gradient\n",
        "    def lipgrad(self):\n",
        "        if self.loss=='l2':\n",
        "            L = norm(self.X, ord=2) ** 2 / self.n + self.lbda\n",
        "        elif self.loss=='logit':\n",
        "            L = norm(self.X, ord=2) ** 2 / (4. * self.n) + self.lbda\n",
        "        return L\n",
        "\n",
        "    # \"Strong\" convexity constant (possibly zero if self.lbda=0)\n",
        "    def cvxval(self):\n",
        "        if self.loss=='l2':\n",
        "            s = svdvals(self.X)\n",
        "            mu = min(s)**2 / self.n\n",
        "            return mu + self.lbda\n",
        "        elif self.loss=='logit':\n",
        "            return self.lbda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY5J6pwGsVCc"
      },
      "source": [
        "We now generate one instance of linear regression and one instance of logistic regression. Note that in our setting, we use $$d=50 \\ll n=1000.$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HaZNvTCMsVCc"
      },
      "outputs": [],
      "source": [
        "# Instance generation\n",
        "d = 50\n",
        "n = 1000\n",
        "idx = np.arange(d)\n",
        "lbda = 1. / n ** (0.5)\n",
        "\n",
        "# Fix random seed for reproducibility\n",
        "np.random.seed(0)\n",
        "\n",
        "# Ground truth\n",
        "w_model_truth = (-1)**idx * np.exp(-idx / 10.)\n",
        "\n",
        "Xlin, ylin = simu_lin(w_model_truth, n, std=1., corr=0.1)\n",
        "Xlog, ylog = simu_lin(w_model_truth, n, std=1., corr=0.7)\n",
        "ylog = np.sign(ylog) # Logarithm for binary classification\n",
        "\n",
        "pblinreg = RegPb(Xlin, ylin,lbda,loss='l2')\n",
        "pblogreg = RegPb(Xlog, ylog,lbda,loss='logit')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VaPQDugsVCc"
      },
      "source": [
        "Given the dimensions of the problem ($d=50,n=1000$), we can apply a powerful optimization technique to obtain a target optimal value and a target optimal point. *Recall that in a big data setting, this method would be too expensive to use.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VnQMxG3YsVCc",
        "outputId": "2c4a8dd3-80cc-477f-850e-4fc576195301",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear regression:\n",
            "\t Numerical minimal value: 0.5609973667736611\n",
            "\t Numerical minimum gradient norm: 5.238598430060513e-09\n",
            "Logistic regression:\n",
            "\t Numerical minimal value: 0.5621593875942845\n",
            "\t Numerical minimum gradient norm: 2.576247290912413e-09\n"
          ]
        }
      ],
      "source": [
        "# Use L-BFGS to get a good approximation for the minimum\n",
        "\n",
        "w_init = np.zeros(d)\n",
        "\n",
        "# Compute a solution for linear regression using L-BFGS\n",
        "w_min_lin, f_min_lin, _ = fmin_l_bfgs_b(pblinreg.fun, w_init, pblinreg.grad, args=(), pgtol=1e-30, factr =1e-30)\n",
        "print(\"Linear regression:\")\n",
        "print(\"\\t Numerical minimal value:\",f_min_lin)\n",
        "print(\"\\t Numerical minimum gradient norm:\",norm(pblinreg.grad(w_min_lin)))\n",
        "# Compute a solution for logistic regression using L-BFGS\n",
        "w_min_log, f_min_log, _ = fmin_l_bfgs_b(pblogreg.fun, w_init, pblogreg.grad, args=(), pgtol=1e-30, factr =1e-30)\n",
        "print(\"Logistic regression:\")\n",
        "print(\"\\t Numerical minimal value:\",f_min_log)\n",
        "print(\"\\t Numerical minimum gradient norm:\",norm(pblogreg.grad(w_min_log)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbNaVpT6sVCd"
      },
      "source": [
        "Letting $f^*$ and $\\mathbf{w}^*$ denote the values computed via L-BFGS, and letting $\\{\\mathbf{w}_k\\}$ denote the iterate sequence of a given method, we will be interested in the behavior of\n",
        "$f(\\mathbf{w}_k)-f^*$ and $\\|\\mathbf{w}_k -\\mathbf{w}^*\\|$ as $k$ increases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKfEi0BXsVCd"
      },
      "source": [
        "# <span style=\"color:rgb(92,29,79)\">Part 2 - Stochastic gradient methods</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFqTZyfvsVCd"
      },
      "source": [
        "In this part, we build a generic stochastic gradient method that will allow for comparing several variants thereof."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EVjkiNhsVCd"
      },
      "source": [
        "## <span style=\"color:rgb(92,29,79)\"> 2.1 Basic stochastic gradient framework</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmRHv1cqsVCd"
      },
      "source": [
        "One iteration of stochastic gradient (also called *Stochastic Gradient Descent* or *SGD*) is given by\n",
        "$$\n",
        "    \\mathbf{w}_{k+1} = \\mathbf{w}_k - \\alpha_k \\nabla f_{i_k}(\\mathbf{w}_k),\n",
        "$$\n",
        "where $i_k$ is drawn at random in $\\{1,\\dots,n\\}$. In this notebook, we will draw $i_k$ according to a uniform distribution (i.e. every value has probability $\\tfrac{1}{n}$ of being drawn).\n",
        "\n",
        "A more general version of this method called *batch stochastic gradient* is given by\n",
        "$$\n",
        "    \\mathbf{w}_{k+1} = \\mathbf{w}_k - \\frac{\\alpha_k}{|\\mathcal{S}_k|} \\sum_{i \\in \\mathcal{S}_k} \\nabla f_i(\\mathbf{w}_k)\n",
        "$$\n",
        "where $\\mathcal{S}_k$ is a set of indices randomly drawn in $\\{1,\\dots,n\\}$. In this notebook, we will consider both draws with and withour replacement. Drawing a subset of cardinality $|\\mathcal{S}_k|=n$ corresponds to performing gradient descent, while using $|\\mathcal{S}_k|=1$ (with or without replacement at the iteration) corresponds to standard stochastic gradient. For simplicity, we will focus on using a constant batch size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJAxwex2sVCe"
      },
      "source": [
        "### <span style=\"color:rgb(92,29,79)\">Question 1</span>\n",
        "\n",
        "*Recall the cost of a batch stochastic gradient iteration in terms of accesses to data points.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is_wbU27sVCe"
      },
      "source": [
        "#### <span style=\"color:rgb(92,29,79)\">Answer to question 1</span>\n",
        "\n",
        "1 iteration = $\\frac{|\\mathcal{S}_k|}{n}$ epoch = $|\\mathcal{S}_k|$ accesses to a data point"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-vgkx4gsVCe"
      },
      "source": [
        "### <span style=\"color:rgb(92,29,79)\">Question 2</span>\n",
        "\n",
        "Recall the definition of an epoch. How does this notion allows for comparing different variants of batch stochastic gradient?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPBWN3I2sVCe"
      },
      "source": [
        "#### <span style=\"color:rgb(92,29,79)\">Answer to question 2</span>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aFmdT1ssVCe"
      },
      "source": [
        "### <span style=\"color:rgb(92,29,79)\">Question 3</span>\n",
        "\n",
        "The code below implements stochastic gradient (in batch version) with two possible choises for the stepsize\n",
        " - $\\alpha_k=\\tfrac{1}{L}$ (constant);\n",
        " - $\\alpha_k=\\frac{\\alpha_0}{(k+1)^t}$, where $\\alpha_0$ and $t>0$ (decreasing).\n",
        "\n",
        "***Fill out the code block below.***\n",
        "\n",
        "*Useful NumPy function:* ``np.random.choice(n,m,replace=False)`` draws $m$ values uniformly in $\\{1,\\dots,n\\}$ without replacement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tR74mfa0sVCe"
      },
      "outputs": [],
      "source": [
        "# Imlementation of stochastic gradient techniques\n",
        "def stoch_grad(w0,problem,wtarget,stepchoice=0,step0=1, n_iter=1000,nb=1,with_replace=False,verbose=False):\n",
        "    \"\"\"\n",
        "        Stochastic gradient framework.\n",
        "\n",
        "        Inputs:\n",
        "            w0: Initial vector\n",
        "            problem: Problem structure\n",
        "                problem.fun() is the objective function (a finite sum)\n",
        "                problem.n returns the number of terms within the sum\n",
        "                problem.grad_i() returns the gradient of a single component f_i\n",
        "                problem.lipgrad() returns a Lipschitz constant for the gradient\n",
        "                problem.cvxval() returns the strong convexity constant for the problem\n",
        "                problem.lambda returns the regularization parameter\n",
        "            wtarget: Target value\n",
        "            stepchoice: Stepsize/Learning rate strategy\n",
        "                0: Constant stepsize equal to 1/L\n",
        "                t>0: Stepsize proportional to 1/(k+1)**t\n",
        "            step0: Initial stepsize (used when stepchoice>0)\n",
        "            n_iter: Number of iterations\n",
        "            nb: Batch size (Default: 1)\n",
        "            with_replace: Drawing indices with/without replacement\n",
        "                True: With replacement\n",
        "                False: Without replacement (Default)\n",
        "            verbose: Boolean indication whether epoch-related info are to be displayedées à chaque epoch (default : False)\n",
        "\n",
        "        Outputs :\n",
        "            w_output: Final iterate\n",
        "            objvals: History of function values (NumPy array of length n_iter)\n",
        "            distits: History of distance to optimal point (NumPy array of length n_iter)\n",
        "    \"\"\"\n",
        "    ############\n",
        "    # Initialization\n",
        "\n",
        "    # History of function values and distance to a theoretical optimum\n",
        "    objvals = []\n",
        "    normits = []\n",
        "\n",
        "    # Lipschitz constant\n",
        "    L = problem.lipgrad()\n",
        "\n",
        "    # Size of the dataset\n",
        "    n = problem.n\n",
        "\n",
        "    # Initial value for the iterate\n",
        "    w = w0.copy()\n",
        "    nw = norm(w)\n",
        "\n",
        "    # Iteration\n",
        "    k=0\n",
        "\n",
        "    # Initial quantities of interest\n",
        "    obj = problem.fun(w)\n",
        "    objvals.append(obj);\n",
        "    nmin = norm(w-wtarget)\n",
        "    normits.append(nmin)\n",
        "\n",
        "    if verbose:\n",
        "        # Plotting info if required\n",
        "        print(\"Stochastic Gradient, batch size=\",nb,\"/\",n)\n",
        "        print(' | '.join([name.center(8) for name in [\"iter\", \"fval\", \"normit\"]]))\n",
        "        print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8),(\"%.2e\" % nmin).rjust(8)]))\n",
        "\n",
        "    ################\n",
        "    # Main loop\n",
        "    while (k < n_iter and nw < 10**100):\n",
        "\n",
        "        ########\n",
        "        # THIS SECTION SHOULD BE FILLED OUT\n",
        "\n",
        "        # Draw random index or indices\n",
        "        # sk = []\n",
        "        # for i in range(nb):\n",
        "        #   sk.append(choice(range(n)))\n",
        "\n",
        "        # Compute the stochastic gradient approximation\n",
        "        sg =\n",
        "        # END PART TO BE FILLED OUT\n",
        "        #######\n",
        "\n",
        "        if stepchoice==0:\n",
        "            w[:] = w - (step0/L) * sg\n",
        "        elif stepchoice>0:\n",
        "            sk = float(step0/((k+1)**stepchoice))\n",
        "            w[:] = w - sk * sg\n",
        "\n",
        "        nw = norm(w) #Compute the norm of the iterate (prevents divergence)\n",
        "\n",
        "        obj = problem.fun(w)\n",
        "        nmin = norm(w-wtarget)\n",
        "\n",
        "        k += 1\n",
        "        # Compute quantities of interest at the end of every epoch\n",
        "        if (k*nb) % n == 0:\n",
        "            objvals.append(obj)\n",
        "            normits.append(nmin)\n",
        "            if verbose:\n",
        "                print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8),(\"%.2e\" % nmin).rjust(8)]))\n",
        "\n",
        "    # End main loop\n",
        "    #################\n",
        "\n",
        "    # Final information\n",
        "    if (k*nb) % n > 0:\n",
        "        objvals.append(obj)\n",
        "        normits.append(nmin)\n",
        "        if verbose:\n",
        "            print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8),(\"%.2e\" % nmin).rjust(8)]))\n",
        "\n",
        "    # Outputs\n",
        "    w_output = w.copy()\n",
        "\n",
        "    return w_output, np.array(objvals), np.array(normits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-47aD4NxsVCf"
      },
      "source": [
        "## <span style=\"color:rgb(92,29,79)\"> 2.2 Gradient descent VS stochastic gradient</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hruGrYvsVCf"
      },
      "source": [
        "The script below compares stochastic gradient and gradient descent on logistic regression given a budget of 60 epochs. For both methods, two stepsize choices are considered: $\\alpha_k = \\tfrac{1}{L}$ and $\\alpha_k = \\tfrac{0.2}{\\sqrt{k+1}}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGQBBjResVCg"
      },
      "outputs": [],
      "source": [
        "# Comparison gradient descent/stochastic gradient\n",
        "# Note: the number of iterations of each method depends on the cost\n",
        "# of a method's iteration\n",
        "nb_epochs = 60\n",
        "n = pblinreg.n\n",
        "nbset = 1\n",
        "w0 = np.zeros(d)\n",
        "\n",
        "# Run a - Gradient descent with constant stepsize\n",
        "w_a, obj_a, nits_a = stoch_grad(w0,pblogreg,w_min_log,stepchoice=0,step0=1, n_iter=nb_epochs,nb=n)\n",
        "\n",
        "# Run b - Stochastic gradient with constant stepsize\n",
        "# This instance can diverge, hence the use of a bound on norm(w)\n",
        "w_b, obj_b, nits_b = stoch_grad(w0,pblogreg,w_min_log,stepchoice=0,step0=1, n_iter=int(nb_epochs*n/nbset),nb=1)\n",
        "\n",
        "# Run c - Gradient descent with decreasing stepsize\n",
        "w_c, obj_c, nits_c = stoch_grad(w0,pblogreg,w_min_log,stepchoice=0.5,step0=0.2, n_iter=nb_epochs,nb=n)\n",
        "# Run d - Stochastic gradient with decreasing stepsize\n",
        "w_d, obj_d, nits_d = stoch_grad(w0,pblogreg,w_min_log,stepchoice=0.5,step0=0.2, n_iter=nb_epochs*n,nb=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F40KG478sVCg"
      },
      "outputs": [],
      "source": [
        "# Plot the behavior of the four methods\n",
        "# The x-axis corresponds to the number of epochs\n",
        "# The y-axis corresponds to the objective function (semilog scale)\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.semilogy(obj_a-f_min_log, label=\"GD - 1/L\", lw=2)\n",
        "plt.semilogy(obj_b-f_min_log, label=\"SG - 1/L\", lw=2)\n",
        "plt.semilogy(obj_c-f_min_log, label=\"GD - 1/sqrt(k+1)\", lw=2)\n",
        "plt.semilogy(obj_d-f_min_log, label=\"SG - 1/sqrt(k+1)\", lw=2)\n",
        "plt.title(\"Convergence\", fontsize=16)\n",
        "plt.xlabel(\"#epochs\", fontsize=14)\n",
        "plt.ylabel(\"Objective (log)\", fontsize=14)\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZQUg9NSsVCg"
      },
      "source": [
        "### <span style=\"color:rgb(92,29,79)\">Question 4</span>\n",
        "\n",
        "*According to the theoretical analysis, how can we explain the behavior of both stochastic gradient variants?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfAUtwuhsVCg"
      },
      "source": [
        "#### <span style=\"color:rgb(92,29,79)\">Answer to question 4</span>\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-YDgXrhsVCg"
      },
      "source": [
        "## <span style=\"color:rgb(92,29,79)\"> 2.3 Stochastic gradient and stepsize</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ihh7AY0PsVCh"
      },
      "source": [
        "One of the key challenges in stochastic gradient is the tuning of the stepsize (also called *learning rate* in machine learning). The script below compares several values that are proportional to $\\frac{1}{L}$, the canonical choice for gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AeubPC_sVCh"
      },
      "outputs": [],
      "source": [
        "# Comparison of several constant learning rate choices\n",
        "\n",
        "nb_epochs = 60\n",
        "n = pblinreg.n\n",
        "nbset = 1\n",
        "w0 = np.zeros(d)\n",
        "\n",
        "valsstep0 = [1,0.1,0.01,0.001,0.0001]\n",
        "nvals = len(valsstep0)\n",
        "\n",
        "objs = np.zeros((nb_epochs+1,nvals))\n",
        "\n",
        "for val in range(nvals):\n",
        "    _, objs[:,val], _ = stoch_grad(w0,pblogreg,w_min_log,stepchoice=0,step0=valsstep0[val], n_iter=int(nb_epochs*n/nbset),nb=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnRVPNP4sVCh"
      },
      "outputs": [],
      "source": [
        "# Plot the behavior of all methods\n",
        "# The x-axis corresponds to the number of epochs\n",
        "# The y-axis corresponds to the objective function (semilog scale)\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.set_cmap(\"RdPu\")\n",
        "for val in range(nvals):\n",
        "    plt.semilogy(objs[:,val]-f_min_log, label=\"SG -\"+str(valsstep0[val])+\"/L\", lw=2)\n",
        "plt.title(\"Convergence\", fontsize=16)\n",
        "plt.xlabel(\"#epochs\", fontsize=14)\n",
        "plt.ylabel(\"Objective (log)\", fontsize=14)\n",
        "plt.legend(loc=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TQgfbDFsVCi"
      },
      "source": [
        "We also compare several choices for a decreasing stepsize.\n",
        "\n",
        "Starting from $\\alpha_0>0$, we also consider $\\tfrac{\\alpha_0}{\\sqrt{k+1}}$, $\\tfrac{\\alpha_0}{k+1}$, $\\tfrac{\\alpha_0}{(k+1)^{1/4}}$, $\\tfrac{\\alpha_0}{(k+1)^{2/3}}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziADpoVvsVCi"
      },
      "outputs": [],
      "source": [
        "# Comparison of several decreasing learning rate choices\n",
        "\n",
        "nb_epochs = 60\n",
        "n = pblinreg.n\n",
        "nbset = 1\n",
        "w0 = np.zeros(d)\n",
        "\n",
        "decstep = [1,2/3,1/2,1/4,0]\n",
        "nvals = len(decstep)\n",
        "\n",
        "objs = np.zeros((nb_epochs+1,nvals))\n",
        "\n",
        "for val in range(nvals):\n",
        "    _, objs[:,val], _ = stoch_grad(w0,pblogreg,w_min_log,stepchoice=decstep[val],step0=0.2, n_iter=int(nb_epochs*n/nbset),nb=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-7D4HV6sVCi"
      },
      "outputs": [],
      "source": [
        "# Plot the behavior of all methods\n",
        "# The x-axis corresponds to the number of epochs\n",
        "# The y-axis corresponds to the objective function (semilog scale)\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.set_cmap(\"RdPu\")\n",
        "for val in range(nvals):\n",
        "    plt.semilogy(objs[:,val]-f_min_log, label=\"SG -(k+1)^{-\"+str(decstep[val])+\"}\", lw=2)\n",
        "plt.title(\"Convergence\", fontsize=16)\n",
        "plt.xlabel(\"#epochs\", fontsize=14)\n",
        "plt.ylabel(\"Objective (log)\", fontsize=14)\n",
        "plt.legend(loc=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3u_88W_ssVCi"
      },
      "source": [
        "### <span style=\"color:rgb(92,29,79)\">Question 5</span>\n",
        "\n",
        "*Based on what we saw during the lectures, how can we explain these observations?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPz4mz8isVCj"
      },
      "source": [
        "#### <span style=\"color:rgb(92,29,79)\">Answer to question 5</span>\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gDlw4eUsVCj"
      },
      "source": [
        "## <span style=\"color:rgb(92,29,79)\"> 2.4 Choosing the batch size</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wLjUbmbsVCj"
      },
      "source": [
        "The script below compares several versions of batch stochastic gradient with their own batch size $n_b$. We consider $n_b \\in \\left\\{1,\\tfrac{n}{100},\\tfrac{n}{10},\\tfrac{n}{2},n \\right\\}$ in order to cover ($n_b=1$) and gradient descent ($n_b=n$ without replacement)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAO7l2qZsVCj"
      },
      "outputs": [],
      "source": [
        "# Comparison of several batch sizes\n",
        "\n",
        "nb_epochs = 100\n",
        "n = pblogreg.n\n",
        "w0 = np.zeros(d)\n",
        "\n",
        "replace_batch=False\n",
        "\n",
        "# Stochastic gradient (batch size=1)\n",
        "w_a, obj_a, nits_a = stoch_grad(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=nb_epochs*n,nb=1)\n",
        "# Batch stochastic gradient (batch size=n/100)\n",
        "nbset=int(n/100)\n",
        "w_b, obj_b, nits_b = stoch_grad(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=int(nb_epochs*n/nbset),nb=nbset,with_replace=replace_batch)\n",
        "# Batch stochastic gradient (batch size=n/10)\n",
        "nbset=int(n/10)\n",
        "w_c, obj_c, nits_c = stoch_grad(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=int(nb_epochs*n/nbset),nb=nbset,with_replace=replace_batch)\n",
        "# Batch stochastic gradient (batch size=n/2)\n",
        "nbset=int(n/2)\n",
        "w_d, obj_d, nits_d = stoch_grad(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=int(nb_epochs*n/nbset),nb=nbset,with_replace=replace_batch)\n",
        "\n",
        "# Gradient descent (batch size=n, indices drawn without replacement)\n",
        "w_f, obj_f, nits_f = stoch_grad(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=int(nb_epochs),nb=n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2f0MUuusVCk"
      },
      "outputs": [],
      "source": [
        "# Results of the comparison\n",
        "\n",
        "# In terms of objective function (log)\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.semilogy(obj_a-f_min_lin, label=\"SG (batch=1)\", lw=2)\n",
        "plt.semilogy(obj_b-f_min_lin, label=\"Batch SG - n/100\", lw=2)\n",
        "plt.semilogy(obj_c-f_min_lin, label=\"Batch SG - n/10\", lw=2)\n",
        "plt.semilogy(obj_d-f_min_lin, label=\"Batch SG - n/2\", lw=2)\n",
        "plt.semilogy(obj_f-f_min_lin, label=\"GD\", lw=2)\n",
        "plt.title(\"Convergence\", fontsize=16)\n",
        "plt.xlabel(\"#epochs\", fontsize=14)\n",
        "plt.ylabel(\"Objective (log)\", fontsize=14)\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xfXNkb3sVCk"
      },
      "source": [
        "### <span style=\"color:rgb(92,29,79)\">Question 6</span>\n",
        "\n",
        "*Comment on the results, and the various regimes observed as a function of the batch size.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4Z1ZsgssVCk"
      },
      "source": [
        "#### <span style=\"color:rgb(92,29,79)\">Answer to question 6</span>\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLT0DdajsVCk"
      },
      "source": [
        "## <span style=\"color:rgb(92,29,79)\">2.5 Variance reduction and batch approaches</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8cuJcl0sVCl"
      },
      "source": [
        "In this last part, we investigate multiple runs of (batch) stochastic gradient methods. Recall that those methods are random in nature, implying that the outcome of a specific realization depends on a realization of a random process. Our goal is to study the variability of these realizations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUm4BFbmsVCl"
      },
      "source": [
        "### <span style=\"color:rgb(92,29,79)\">Question 7</span>\n",
        "\n",
        "Run the code below to run several runs of batch stochastic gradient with stepsize\n",
        "$\\frac{0.2}{\\sqrt{k+1}}$. Explain how the resulting plot illustrates the variance reduction capabilities of batch stochastic gradient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtxW2shUsVCl"
      },
      "outputs": [],
      "source": [
        "# Test several values for the batch size using the same epoch budget.\n",
        "\n",
        "nb_epochs = 100\n",
        "n = pblinreg.n\n",
        "w0 = np.zeros(d)\n",
        "\n",
        "nruns = 10\n",
        "\n",
        "for i in range(nruns):\n",
        "    # Run standard stochastic gradient (batch size 1)\n",
        "    _, obj_a, _ = stoch_grad(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=nb_epochs*n,nb=1,with_replace=True)\n",
        "    # Batch stochastic gradient (batch size n/10)\n",
        "    nbset=int(n/10)\n",
        "    _, obj_b, _ = stoch_grad(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=int(nb_epochs*n/nbset),nb=nbset,with_replace=True)\n",
        "    # Batch stochastic gradient (batch size n/2)\n",
        "    nbset=int(n/2)\n",
        "    _, obj_c, _ = stoch_grad(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=int(nb_epochs*n/nbset),nb=nbset,with_replace=True)\n",
        "    # Batch stochastic gradient (batch size n, with replacement)\n",
        "    nbset=n\n",
        "    _, obj_d, _ = stoch_grad(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=int(nb_epochs*n/nbset),nb=nbset,with_replace=True)\n",
        "\n",
        "    # Plots runs on the same figure\n",
        "    if i<nruns-1:\n",
        "        plt.semilogy(obj_a-f_min_lin,color='orange',lw=2)\n",
        "        plt.semilogy(obj_b-f_min_lin,color='green', lw=2)\n",
        "        plt.semilogy(obj_c-f_min_lin,color='red', lw=2)\n",
        "        plt.semilogy(obj_d-f_min_lin,color='blue', lw=2)\n",
        "plt.semilogy(obj_a-f_min_lin,label=\"SG\",color='orange',lw=2)\n",
        "plt.semilogy(obj_b-f_min_lin,label=\"batch n/10\",color='green',lw=2)\n",
        "plt.semilogy(obj_c-f_min_lin,label=\"batch n/2\",color='red', lw=2)\n",
        "plt.semilogy(obj_d-f_min_lin,label=\"batch n\",color='blue', lw=2)\n",
        "\n",
        "plt.title(\"Convergence plot\", fontsize=16)\n",
        "plt.xlabel(\"#epochs (log scale)\", fontsize=14)\n",
        "plt.ylabel(\"Objective (log scale)\", fontsize=14)\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkM_dO6esVCl"
      },
      "source": [
        "#### <span style=\"color:rgb(92,29,79)\">Answer to question 7</span>\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpiiJZyfsVCm"
      },
      "source": [
        "# <span style=\"color:rgb(92,29,79)\">Part 3 - Advanced variants of stochastic gradient</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2Pz2EZ5sVCm"
      },
      "source": [
        "The goal of this section is to present popular variants of the classical stochastic gradient scheme. The augmented code below aims at allowing iterate averaging and diagonal scaling to the method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR2DS4cgsVCm"
      },
      "source": [
        "##  <span style=\"color:rgb(92,29,79)\">3.1 SG variants based on diagonal scaling</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkG-sYD4sVCn"
      },
      "source": [
        "#### <span style=\"color:rgb(92,29,79)\">A description of *RMSProp* and *Adagrad*</span>\n",
        "\n",
        "*RMSProp* and *Adagrad* are both based on diagonal scaling. This corresponds to rescaling the stochastic gradient step componentwise as follows\n",
        " $$\n",
        "     [\\mathbf{w}_{k+1}]_i  = [\\mathbf{w}_k]_i -\\frac{\\alpha}{\\sqrt{[\\mathbf{r}_k]_i + \\epsilon}}[\\nabla f_{i_k}(\\mathbf{w}_k)]_i,\n",
        " $$\n",
        " where $\\epsilon>0$ is added to avoid numerical issues, and $\\mathbf{r}_k \\in \\mathbb{R}^d$ is defined recursively by $\\mathbf{r}_{-1} = 0_{\\mathbb{R}^d}$ and\n",
        " $$\n",
        "     \\forall k \\ge 0,\\ \\forall i=1,\\dots,d, \\qquad\n",
        "     [\\mathbf{r}_k]_i =\n",
        "     \\left\\{\n",
        "         \\begin{array}{ll}\n",
        "             \\beta_2 [\\mathbf{r}_{k-1}]_i + (1-\\beta_2) [\\nabla f_{i_k}(\\mathbf{w}_k)]_i^2 &\\mathrm{for\\ RMSProp,} \\\\\n",
        "             [\\mathbf{r}_{k-1}]_i + [\\nabla f_{i_k}(\\mathbf{w}_k)]_i^2 &\\mathrm{for\\ Adagrad.}\n",
        "         \\end{array}\n",
        "     \\right.\n",
        " $$\n",
        "(Suggested values: $\\epsilon=10^{-8}$, $\\beta=0.8$.) The use of $\\epsilon>0$ prevents the scaling of each component of the stochastic gradient from going to zero. *This technique is typically adopted in modern implementations of these methods.*\n",
        "\n",
        "Batch variants of Adagrad and RMSProp can be obtained by replacing $\\nabla f_{i_k}(\\mathbf{w}_k)$ by a batch gradient estimate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIbenRc4sVCn"
      },
      "source": [
        "### <span style=\"color:rgb(92,29,79)\">Question 8</span>\n",
        "\n",
        "Fill out the code below with your implementation from question 1 as well as the diagonal scaling feature as done in RMSProp and Adagrad. To this end, use the parameter `scaling`, equal to $1$ for Adagrad, and to $\\beta_2$ for RMSProp."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5KoqXo1sVCn"
      },
      "outputs": [],
      "source": [
        "# Advanced stochastic gradient implementation\n",
        "def adv_stoch_grad(w0,problem,wtarget,stepchoice=0,step0=1, n_iter=1000,nb=1,scaling=0,with_replace=False,verbose=False):\n",
        "    \"\"\"\n",
        "        A code for gradient descent with various step choices.\n",
        "\n",
        "        Inputs:\n",
        "            w0: Initial vector\n",
        "            problem: Problem structure\n",
        "                problem.fun() returns the objective function, which is assumed to be a finite sum of functions\n",
        "                problem.n returns the number of components in the finite sum\n",
        "                problem.grad_i() returns the gradient of a single component f_i\n",
        "                problem.lipgrad() returns the Lipschitz constant for the gradient\n",
        "                problem.cvxval() returns the strong convexity constant\n",
        "                problem.lambda returns the value of the regularization parameter\n",
        "            wtarget: Target minimum (unknown in practice!)\n",
        "            stepchoice: Strategy for computing the stepsize\n",
        "                0: Constant step size equal to 1/L\n",
        "                t>0: Step size decreasing in 1/(k+1)^t\n",
        "            step0: Initial steplength (only used when stepchoice is not 0)\n",
        "            n_iter: Number of iterations, used as stopping criterion\n",
        "            nb: Number of components drawn per iteration/Batch size\n",
        "                1: Classical stochastic gradient algorithm (default value)\n",
        "                problem.n: Classical gradient descent (default value)\n",
        "            scaling: Use a diagonal scaling\n",
        "                0: No scaling (default)\n",
        "                (0,1): Average of magnitudes (RMSProp)\n",
        "                1: Normalization with magnitudes (Adagrad)\n",
        "            with_replace: Boolean indicating whether components are drawn with or without replacement\n",
        "                True: Components drawn with replacement\n",
        "                False: Components drawn without replacement (Default)\n",
        "            verbose: Boolean indicating whether information should be plot at every iteration (Default: False)\n",
        "\n",
        "        Outputs:\n",
        "            w_output: Final iterate of the method (or average if average=1)\n",
        "            objvals: History of function values (Numpy array of length n_iter at most)\n",
        "            normits: History of distances between iterates and optimum (Numpy array of length n_iter at most)\n",
        "    \"\"\"\n",
        "    ############\n",
        "    # Initial step: Compute and plot some initial quantities\n",
        "\n",
        "    # objective history\n",
        "    objvals = []\n",
        "\n",
        "    # iterates distance to the minimum history\n",
        "    normits = []\n",
        "\n",
        "    # Lipschitz constant\n",
        "    L = problem.lipgrad()\n",
        "\n",
        "    # Number of samples\n",
        "    n = problem.n\n",
        "\n",
        "    # Initial value of current iterate\n",
        "    w = w0.copy()\n",
        "    nw = norm(w)\n",
        "\n",
        "    #Scaling values\n",
        "    if scaling>0:\n",
        "        eps=10**(-8) # To avoid numerical issues\n",
        "        v = np.zeros(d)\n",
        "\n",
        "    # Initialize iteration counter\n",
        "    k=0\n",
        "\n",
        "    # Current objective\n",
        "    obj = problem.fun(w)\n",
        "    objvals.append(obj);\n",
        "    # Current distance to the optimum\n",
        "    nmin = norm(w-wtarget)\n",
        "    normits.append(nmin)\n",
        "\n",
        "    # Plot initial quantities of interest\n",
        "    if verbose:\n",
        "        print(\"Stochastic Gradient, batch size=\",nb,\"/\",n)\n",
        "        print(' | '.join([name.center(8) for name in [\"iter\", \"fval\", \"normit\"]]))\n",
        "        print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8),(\"%.2e\" % nmin).rjust(8)]))\n",
        "\n",
        "    ################\n",
        "    # Main loop\n",
        "    while (k < n_iter and nw < 10**100):\n",
        "\n",
        "        #########################################\n",
        "        # FILL OUT THIS PART WITH YOUR IMPLEMENTATION FROM QUESTION 1\n",
        "\n",
        "        # Draw the batch indices\n",
        "        # Stochastic gradient calculation\n",
        "        sg =\n",
        "        ###########################################\n",
        "\n",
        "        ###########################################\n",
        "        # ADD THE SCALING FEATURE\n",
        "        if scaling>0:\n",
        "            if scaling==1:\n",
        "                # Adagrad update\n",
        "                v =\n",
        "            else:\n",
        "                # RMSProp update\n",
        "                v =\n",
        "            sg =\n",
        "        ##########################################\n",
        "\n",
        "        if stepchoice==0:\n",
        "            w[:] = w - (step0/L) * sg\n",
        "        elif stepchoice>0:\n",
        "            sk = float(step0/((k+1)**stepchoice))\n",
        "            w[:] = w - sk * sg\n",
        "\n",
        "        nw = norm(w) #Computing the norm to measure divergence\n",
        "\n",
        "\n",
        "        obj = problem.fun(w)\n",
        "        nmin = norm(w-wtarget)\n",
        "\n",
        "\n",
        "        k += 1\n",
        "        # Plot quantities of interest at the end of every epoch only\n",
        "        if (k*nb) % n == 0:\n",
        "            objvals.append(obj)\n",
        "            normits.append(nmin)\n",
        "            if verbose:\n",
        "                print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8),(\"%.2e\" % nmin).rjust(8)]))\n",
        "\n",
        "    # End of main loop\n",
        "    #################\n",
        "\n",
        "    # Plot quantities of interest for the last iterate (if needed)\n",
        "    if (k*nb) % n > 0:\n",
        "        objvals.append(obj)\n",
        "        normits.append(nmin)\n",
        "        if verbose:\n",
        "            print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8),(\"%.2e\" % nmin).rjust(8)]))\n",
        "\n",
        "    # Outputs\n",
        "    w_output = w.copy()\n",
        "\n",
        "    return w_output, np.array(objvals), np.array(normits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q9N2TfSsVCo"
      },
      "source": [
        "### <span style=\"color:rgb(92,29,79)\">Question 9</span>\n",
        "\n",
        "Run the script below to compare RMSProp and Adagrad using a decreasing stepsize $\\frac{\\alpha_0}{\\sqrt{k+1}}$ or a constant stepsize with SG (use a decreasing stepsize for SG). What do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFxcbCSPsVCp"
      },
      "outputs": [],
      "source": [
        "# Question 9 - Comparison of stochastic gradient with and without diagonal scaling\n",
        "\n",
        "nb_epochs = 60\n",
        "n = pblinreg.n\n",
        "w0 = np.zeros(d)\n",
        "\n",
        "# Stochastic gradient (batch size 1) without diagonal scaling\n",
        "w_a, obj_a, nits_a = adv_stoch_grad(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=nb_epochs*n,nb=1)\n",
        "# Stochastic gradient (batch size 1) with Adagrad diagonal scaling (decreasing stepsize)\n",
        "w_bd, obj_bd, nits_bd = adv_stoch_grad(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=nb_epochs*n,nb=1,scaling=1)\n",
        "# Stochastic gradient (batch size 1) with RMSProp diagonal scaling (decreasing stepsize)\n",
        "w_cd, obj_cd, nits_cd = adv_stoch_grad(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=nb_epochs*n,nb=1,scaling=0.8)\n",
        "# Stochastic gradient (batch size 1) with Adagrad diagonal scaling (constant stepsize)\n",
        "w_bc, obj_bc, nits_bc = adv_stoch_grad(w0,pblinreg,w_min_lin,stepchoice=0,step0=0.2, n_iter=nb_epochs*n,nb=1,scaling=1)\n",
        "# Stochastic gradient (batch size 1) with RMSProp diagonal scaling (constant stepsize)\n",
        "w_cc, obj_cc, nits_cc = adv_stoch_grad(w0,pblinreg,w_min_lin,stepchoice=0,step0=0.2, n_iter=nb_epochs*n,nb=1,scaling=0.8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-P1G3RwsVCp"
      },
      "outputs": [],
      "source": [
        "# Plot the results - Comparison of stochastic gradient with and without diagonal scaling\n",
        "# In terms of objective value (logarithmic scale)\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.semilogy(obj_a-f_min_lin, label=\"SG\", lw=2)\n",
        "plt.semilogy(obj_bd-f_min_lin, label=\"Adagrad (dec)\", lw=2)\n",
        "plt.semilogy(obj_cd-f_min_lin, label=\"RMSProp (dec)\", lw=2)\n",
        "plt.semilogy(obj_bc-f_min_lin, label=\"Adagrad (cst)\", lw=2)\n",
        "plt.semilogy(obj_cc-f_min_lin, label=\"RMSProp (cst)\", lw=2)\n",
        "plt.title(\"SG vs Adagrad/RMSProp\", fontsize=16)\n",
        "plt.xlabel(\"#epochs (log scale)\", fontsize=14)\n",
        "plt.ylabel(\"Objective (log scale)\", fontsize=14)\n",
        "plt.legend()\n",
        "# In terms of distance to the minimum (logarithmic scale)\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.semilogy(nits_a, label=\"SG\", lw=2)\n",
        "plt.semilogy(nits_bd, label=\"Adagrad (dec)\", lw=2)\n",
        "plt.semilogy(nits_cd, label=\"RMSProp (dec)\", lw=2)\n",
        "plt.semilogy(nits_bc, label=\"Adagrad (cst)\", lw=2)\n",
        "plt.semilogy(nits_cc, label=\"RMSProp (cst)\", lw=2)\n",
        "plt.title(\"SG vs Adagrad/RMSProp\", fontsize=16)\n",
        "plt.xlabel(\"#epochs\", fontsize=14)\n",
        "plt.ylabel(\"Distance to minimum (log scale)\", fontsize=14)\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMcVv59YsVCp"
      },
      "source": [
        "#### <span style=\"color:rgb(92,29,79)\">Answer to question 9</span>\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4kS-35osVCq"
      },
      "source": [
        "## <span style=\"color:rgb(92,29,79)\">3.2 Stochastic gradient with momentum</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSD7765OsVCq"
      },
      "source": [
        "#### <span style=\"color:rgb(92,29,79)\">A description of *SGD with momentum* and *Adam*</span>\n",
        "\n",
        "Certain variants of stochastic gradient exploit the idea of momentum, i.e. they combine a classical stochastic gradient steps with steps from previous iterations.\n",
        "\n",
        "*Stochastic gradient with momentum* can be viewed as the stochastic version of the heavy-ball method, and consists in the following iteration\n",
        "$$\n",
        "    \\mathbf{w}_{k+1} = \\mathbf{w}_k - \\alpha_k \\mathbf{m}_k,\n",
        "    \\quad \\mathrm{where} \\quad\n",
        "    \\mathbf{m}_k = \\beta_1 \\mathbf{m}_{k-1} + (1-\\beta_1)\\nabla f_{i_k}(\\mathbf{w}_k)\n",
        "$$\n",
        "where $\\beta_1 \\in (0,1)$ (using $\\beta_1=0$ would give the standard stochastic gradient method).\n",
        "\n",
        "*Adam* combines the momentum principle with diagonal scaling. That method accumulates information through all iterations. The update can be written as\n",
        "$$\n",
        "    \\mathbf{w}_{k+1} = \\mathbf{w}_k - \\alpha_k \\mathbf{m}_k \\oslash \\sqrt{\\mathbf{v}_k},\n",
        "$$\n",
        "where\n",
        "$$\n",
        "    \\mathbf{m}_k = \\frac{1-\\beta_1^k}{1-\\beta_1^{k+1}}\\beta_1\\mathbf{m}_{k-1} + \\frac{1-\\beta_1}{1-\\beta_1^{k+1}}\\nabla f_{i_k}(\\mathbf{w}_k)\n",
        "$$\n",
        "and\n",
        "$$\n",
        "    \\mathbf{v}_k = \\frac{1-\\beta_2^k}{1-\\beta_2^{k+1}}\\beta_2\\mathbf{v}_{k-1} + \\frac{1-\\beta_2}{1-\\beta_2^{k+1}}\\nabla f_{i_k}(\\mathbf{w}_k)\\otimes\\nabla f_{i_k}(\\mathbf{w}_k).\n",
        "$$\n",
        "\n",
        "In practice, as in the previous section, we replace $\\sqrt{\\mathbf{v}_k}$ with $\\sqrt{\\mathbf{v}_k+\\epsilon}$ for numerical stability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqlKsaQxsVCq"
      },
      "source": [
        "### <span style=\"color:rgb(92,29,79)\">Question 10</span>\n",
        "\n",
        "Fill out the code below and run the next block to compare SG with momentum and Adam with\n",
        "basic stochastic gradient. What do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjzoBKJTsVCr"
      },
      "outputs": [],
      "source": [
        "# Stochastic gradient technique with momentum\n",
        "def stoch_grad_momentum(w0,problem,wtarget,stepchoice=0,step0=1, n_iter=1000,nb=1,beta1=0.9,beta2=0.999,with_replace=False,verbose=False):\n",
        "    \"\"\"\n",
        "        A code for stochastic gradient with various step choices.\n",
        "\n",
        "        Inputs:\n",
        "            w0: Initial vector\n",
        "            problem: Problem structure\n",
        "                problem.fun() returns the objective function, which is assumed to be a finite sum of functions\n",
        "                problem.n returns the number of components in the finite sum\n",
        "                problem.grad_i() returns the gradient of a single component f_i\n",
        "                problem.lipgrad() returns the Lipschitz constant for the gradient\n",
        "                problem.cvxval() returns the strong convexity constant\n",
        "                problem.lambda returns the value of the regularization parameter\n",
        "            wtarget: Target minimum (unknown in practice!)\n",
        "            stepchoice: Strategy for computing the stepsize\n",
        "                0: Constant step size equal to 1/L\n",
        "                t>0: Step size decreasing in 1/(k+1)^t\n",
        "            step0: Initial steplength (only used when stepchoice is not 0)\n",
        "            n_iter: Number of iterations, used as stopping criterion\n",
        "            nb: Number of components drawn per iteration/Batch size\n",
        "                1: Classical stochastic gradient algorithm (default value)\n",
        "                problem.n: Classical gradient descent (default value)\n",
        "            beta1: Momentum parameter for the direction\n",
        "                0: Regular stochastic gradient\n",
        "                (0,1): Momentum method (default: 0.9)\n",
        "            beta2: Diagonal scaling parameter\n",
        "                0: No scaling (SGD with momentum)\n",
        "                (0,1): Geometric average (Adam, default: 0.999)\n",
        "            with_replace: Boolean indicating whether components are drawn with or without replacement\n",
        "                True: Components drawn with replacement\n",
        "                False: Components drawn without replacement (Default)\n",
        "            verbose: Boolean indicating whether information should be plot at every iteration (Default: False)\n",
        "\n",
        "        Outputs:\n",
        "            w_output: Final iterate of the method (or average if average=1)\n",
        "            objvals: History of function values (Numpy array of length n_iter at most)\n",
        "            normits: History of distances between iterates and optimum (Numpy array of length n_iter at most)\n",
        "    \"\"\"\n",
        "    ############\n",
        "    # Initial step: Compute and plot some initial quantities\n",
        "\n",
        "    # objective history\n",
        "    objvals = []\n",
        "\n",
        "    # iterates distance to the minimum history\n",
        "    normits = []\n",
        "\n",
        "    # Lipschitz constant\n",
        "    L = problem.lipgrad()\n",
        "\n",
        "    # Number of samples\n",
        "    n = problem.n\n",
        "\n",
        "    # Initial value of current iterate\n",
        "    w = w0.copy()\n",
        "    nw = norm(w)\n",
        "\n",
        "    #Momentum vector\n",
        "    mv = np.zeros(d)\n",
        "\n",
        "    #Scaling values\n",
        "    if beta2>0:\n",
        "        eps=10**(-8) # To avoid numerical issues\n",
        "        v = np.zeros(d)\n",
        "\n",
        "    # Initialize iteration counter\n",
        "    k=0\n",
        "\n",
        "    # Current objective\n",
        "    obj = problem.fun(w)\n",
        "    objvals.append(obj);\n",
        "    # Current distance to the optimum\n",
        "    nmin = norm(w-wtarget)\n",
        "    normits.append(nmin)\n",
        "\n",
        "    # Plot initial quantities of interest\n",
        "    if verbose:\n",
        "        print(\"Stochastic Gradient, batch size=\",nb,\"/\",n)\n",
        "        print(' | '.join([name.center(8) for name in [\"iter\", \"fval\", \"normit\"]]))\n",
        "        print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8),(\"%.2e\" % nmin).rjust(8)]))\n",
        "\n",
        "    ################\n",
        "    # Main loop\n",
        "    while (k < n_iter and nw < 10**100):\n",
        "\n",
        "        #########################################\n",
        "        # FILL OUT THIS PART WITH YOUR IMPLEMENTATION FROM QUESTION 1\n",
        "        # Draw the batch indices\n",
        "\n",
        "        # Stochastic gradient calculation\n",
        "        sg =\n",
        "        ###########################################\n",
        "\n",
        "        ###########################################\n",
        "        # FILL OUT THIS PART WITH THE UPDATE OF MV\n",
        "        if beta1>0:\n",
        "            if beta2>0: # Adam\n",
        "                mv =\n",
        "            else: # SG with momentum\n",
        "                mv =\n",
        "        else: # Basic SG\n",
        "            mv =\n",
        "\n",
        "        ###########################################\n",
        "        # FILL OUT THIS PART WITH THE UPDATE OF V\n",
        "        if beta2>0:\n",
        "            v =\n",
        "            mv =\n",
        "        ##########################################\n",
        "\n",
        "        if stepchoice==0:\n",
        "            w[:] = w - (step0/L) * mv\n",
        "        elif stepchoice>0:\n",
        "            sk = float(step0/((k+1)**stepchoice))\n",
        "            w[:] = w - sk * mv\n",
        "\n",
        "        nw = norm(w) #Computing the norm to measure divergence\n",
        "\n",
        "        obj = problem.fun(w)\n",
        "        nmin = norm(w-wtarget)\n",
        "\n",
        "        k += 1\n",
        "        # Plot quantities of interest at the end of every epoch only\n",
        "        if (k*nb) % n == 0:\n",
        "            objvals.append(obj)\n",
        "            normits.append(nmin)\n",
        "            if verbose:\n",
        "                print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8),(\"%.2e\" % nmin).rjust(8)]))\n",
        "\n",
        "    # End of main loop\n",
        "    #################\n",
        "\n",
        "    # Plot quantities of interest for the last iterate (if needed)\n",
        "    if (k*nb) % n > 0:\n",
        "        objvals.append(obj)\n",
        "        normits.append(nmin)\n",
        "        if verbose:\n",
        "            print(' | '.join([(\"%d\" % k).rjust(8),(\"%.2e\" % obj).rjust(8),(\"%.2e\" % nmin).rjust(8)]))\n",
        "\n",
        "    # Outputs\n",
        "    w_output = w.copy()\n",
        "\n",
        "    return w_output, np.array(objvals), np.array(normits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_G8uubvsVCr"
      },
      "outputs": [],
      "source": [
        "# Test a number of stochastic gradient methods with momentum\n",
        "nb_epochs = 100\n",
        "n = pblinreg.n\n",
        "w0 = np.zeros(d)\n",
        "\n",
        "# Stochastic gradient (decreasing stepsize)\n",
        "x_sg, obj_sg, nits_sg = stoch_grad_momentum(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=nb_epochs*n,nb=1,beta1=0,beta2=0)\n",
        "# SGD with momentum (decreasing stepsize)\n",
        "x_sgm_d, obj_sgm_d, nits_sgm_d = stoch_grad_momentum(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=nb_epochs*n,nb=1,beta1=0.9,beta2=0)\n",
        "# Adam (decreasing stepsize)\n",
        "x_adam_d, obj_adam_d, nits_adam_d = stoch_grad_momentum(w0,pblinreg,w_min_lin,stepchoice=0.5,step0=0.2, n_iter=nb_epochs*n,nb=1)\n",
        "# SGD with momentum (constant stepsize)\n",
        "x_sgm_c, obj_sgm_c, nits_sgm_c = stoch_grad_momentum(w0,pblinreg,w_min_lin,stepchoice=0,step0=0.001, n_iter=nb_epochs*n,nb=1,beta1=0.9,beta2=0)\n",
        "# Adam (constant stepsize)\n",
        "x_adam_c, obj_adam_c, nits_adam_c = stoch_grad_momentum(w0,pblinreg,w_min_lin,stepchoice=0,step0=0.2, n_iter=nb_epochs*n,nb=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9611LqK-sVCr"
      },
      "outputs": [],
      "source": [
        "# Plot the results - Comparison of stochastic gradient with and without momentum\n",
        "# In terms of objective value (logarithmic scale)\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.semilogy(obj_sg-f_min_lin, label=\"SG\", lw=2)\n",
        "plt.semilogy(obj_sgm_d-f_min_lin, label=\"SG+momentum (dec)\", lw=2)\n",
        "plt.semilogy(obj_adam_d-f_min_lin, label=\"Adam (dec)\", lw=2)\n",
        "plt.semilogy(obj_sgm_c-f_min_lin, label=\"SG+momentum (cst)\", lw=2)\n",
        "plt.semilogy(obj_adam_c-f_min_lin, label=\"Adam (cst)\", lw=2)\n",
        "plt.title(\"SG w/o momentum\", fontsize=16)\n",
        "plt.xlabel(\"#epochs (log scale)\", fontsize=14)\n",
        "plt.ylabel(\"Objective (log scale)\", fontsize=14)\n",
        "plt.legend()\n",
        "# In terms of distance to the minimum (logarithmic scale)\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.semilogy(nits_sg, label=\"SG\", lw=2)\n",
        "plt.semilogy(nits_sgm_d, label=\"SG+momentum (dec)\", lw=2)\n",
        "plt.semilogy(nits_adam_d, label=\"Adam (dec)\", lw=2)\n",
        "plt.semilogy(nits_sgm_c, label=\"SG+momentum (cst)\", lw=2)\n",
        "plt.semilogy(nits_adam_c, label=\"Adam (cst)\", lw=2)\n",
        "plt.title(\"SG vs Adagrad/RMSProp\", fontsize=16)\n",
        "plt.xlabel(\"#epochs\", fontsize=14)\n",
        "plt.ylabel(\"Distance to minimum (log scale)\", fontsize=14)\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQVHITUjsVCs"
      },
      "source": [
        "#### <span style=\"color:rgb(92,29,79)\">Answer to question 10</span>\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwjH8v9ysVCs"
      },
      "outputs": [],
      "source": [
        "# Version 4.0 - C. W. Royer, December 2024."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}